{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0817cd",
   "metadata": {},
   "source": [
    "# Assignment 12\n",
    "This exercise has two parts: \n",
    "\n",
    "- Part A consists of mandatory exercises, you need to complete these and hand in the resulting answers to be able to partake in the exam.\n",
    "\n",
    "- Part B consists of additional exercises. They either repeat a concept that is covered in part 1 in a slightly different way, or expand/broaden some topics. \n",
    "\n",
    "In theory, part A should prepare you for (most of) the exam, but some exercises in part B might give you additional insights that both help to complete part A, and do better at the exam. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b40c88",
   "metadata": {},
   "source": [
    "## Assignment by\n",
    "Group: 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a85f3a",
   "metadata": {},
   "source": [
    "## Advised Reading and Exercise Material\n",
    "- TSK Chapter 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db635491",
   "metadata": {},
   "source": [
    "# A: Mandatory Exercises\n",
    "Below you will find the mandatory exercises for week 12 of the course Data Mining and Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb0ec0",
   "metadata": {},
   "source": [
    "## A.1 Support Vector Machines\n",
    "\n",
    "In this exercise we're taking a look at Support Vector Machines.\n",
    "We'll again analyze the XOR data. and look at what some of the hyperparameters do.\n",
    "\n",
    "### A.1.1 Loading the data\n",
    "Load the data in `data/xor.mat` and inspect the shapes to see if everything is alright.\n",
    "\n",
    "**Helpful hints:**\n",
    " + Use scipy.io loadmat, like previous weeks!\n",
    " + The labels are each in a list as well, consider using np.squeeze() to fix that (read the documentation if you forgot what it does!)\n",
    " + The data is stored under `X` (data) and `y` (labels) again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038fbbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(400, 2), y.shape=(400, 1), labels=[0 1]\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "data = loadmat(\"data/xor.mat\")\n",
    "X, y = data[\"X\"], data[\"y\"]\n",
    "\n",
    "print(f'X.shape={X.shape}, y.shape={y.shape}, labels={np.unique(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5f2ef",
   "metadata": {},
   "source": [
    "\n",
    "### A.1.2 Linear kernels\n",
    "We start of relatively simple: Let's just fit a SVM with a linear kernel.\n",
    "\n",
    "- Use a linear kernel support vector machine from Scikit-learn, `SVC`, and train it on the Xor data. \n",
    "- Plot the boundaries of the classification using the `plot_boundaries` function from the toolbox (remember how to do this from previous weeks?)\n",
    "- What do you observe when you use the basic settings, but with a linear kernel? Why does or doesn't the model perform well? \n",
    "- Do you get different results when rerunning the algorithm?\n",
    "\n",
    "**Helpful hints:**\n",
    "- look at the parameter `kernel` for `SVC`\n",
    "- Theory: Is XOR seperated by any linear line in 2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4e747",
   "metadata": {},
   "source": [
    "YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c67af2",
   "metadata": {},
   "source": [
    "### A.1.3 - Theory Question\n",
    "In order to solve the problem a linear kernel SVM has when facing such non-linearly separable data data, we can transform the data using a so called kernel. Several kernels exist, but in this case we'll look into a second degree polynomial kernel first.\n",
    "\n",
    "If you remember from several weeks ago, we can transform some input to a set of polynomial features on that input. We looked into using `PolynomialFeatures` onto 1D data to extract it into the 1st-6th polynomial, and then ran linear regression on it.\n",
    "\n",
    "Let's now look at the example for degree=2 for inputs $(x_1, x_2)$, this would give $$\\mathbf{p}(\\mathbf{x}) = \\mathbf{(1, x_1, x_2, x_1^2, x_2^2, x_1 x_2)}$$\n",
    "\n",
    "However, there are several ways to construct a second degree polynomial kernel, e.g.:\n",
    "- You can include or exclude the bias (the $x_i^0$ term resulting in the 1 in $p(x)$ )\n",
    "- You can include or exclude non-interactions ($x_i^d$) terms\n",
    "\n",
    "**Questions**:\n",
    "1. Why can the XOR problem never be seperated with a linear kernel on $(x_1, x_2)$, give a table with inputs $(x_1, x_2)$ and output $y$ to explain this answer.\n",
    "2. Now add the interaction term $x_1 * x_2$ to the table. Do you think this gives enough information to seperate the classes?\n",
    "3. Are the other parameters necessary? Is it worse to include them (performance, but also runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e8234",
   "metadata": {},
   "source": [
    "YOUR ANSWERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b7e9e",
   "metadata": {},
   "source": [
    "### A.1.3 The kernel trick\n",
    "1. Fit another SVM to the data, but now make sure you apply the kernel trick. Use the knowledge from the theory above to think about what parameters to include!\n",
    "     - Verify you end up with a $400*3$ matrix. Assign this NumPy array to a variable called `KX`.\n",
    "2. Use a **linear** kernel SVM with otherwise default settings to predict `y`. What is the accuracy of the classifier now?\n",
    "3. Plot the 3 dimensional decision boundary\n",
    "4. Plot the 3 dimensional decision boundary again, but now from elev=25 and azim=45\n",
    "5. Does the plot match your expectations?\n",
    "   \n",
    "**Helpful hints:**\n",
    "- For (1), you can do this manually, or using the `PolynomialFeatures` function from Scikit-learn with the correct settings (look at `include_bias` and `interaction_only`)\n",
    "- For (2), you should be able to reuse what you did for A.1.1\n",
    "- For (3,4), look at the function `plot_boundaries_3d` in toolbox/plot_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee333bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af723aa",
   "metadata": {},
   "source": [
    "### A.1.4 Using the kernel trick directly\n",
    "\n",
    "Instead of manually applying the kernel trick, we can also use the polynomial kernel directly when initializing the SVC object. \n",
    "\n",
    "- Apply SVM using a second degree polynomial kernel and calculate and print the score. Save the score to a variable called `second_degree_svm_accuracy2`.\n",
    "- Use the `plot_boundaries` function to show the classification boundaries. \n",
    "- Is the result the same as when you applied the trick manually? Why (not)?\n",
    "\n",
    "**Helpful hints:**\n",
    "- Think about your answer to the theory questions in A.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b24aa",
   "metadata": {},
   "source": [
    "YOUR ANSWERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a88c1",
   "metadata": {},
   "source": [
    "## A.2 Basic anomaly detection\n",
    "In this section, we'll allow you to get acquainted with the basics of anomaly detection. We'll use some of the basic algorithms contained in Scikit-learn for initial analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50653582",
   "metadata": {},
   "source": [
    "### Exercise A.2.1 \n",
    "Load the data from `data/anomaly_1.npz`. This is a numpy zip format. Read the documention on how this format works (np.load). Save the data and labels to variables called X_1 and y_1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0882ee5",
   "metadata": {},
   "source": [
    "### Exercise A.2.2\n",
    "Plot the data in `X_1`. Colour the points based on the label in `y_1`. Add a legend to indicate which color indicates what class. In this legend, the 0 class should be called \"normal\" and the 1 class should be called \"anomaly\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b236b",
   "metadata": {},
   "source": [
    "### Exercise A.2.3\n",
    "Now it's time to apply our first anomaly detection algorithm to the data. Remember, in **unsupervised** learning, we don't have access to labels, so we won't use the `y_1` variable just yet.\n",
    "Fit the IsolationForest algorithm from Scikit-learn on this data. The default parameters will suffice, with the exception of the random state variable, which should be: `random_state=1337`.\n",
    "After fitting, we won't look at the prediction, which is already binarized, but rather to the output of the `score_samples` function. Save the scores resulting from this function to a variable called `y_1_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ecb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490eef27",
   "metadata": {},
   "source": [
    "### Exercise A.2.4\n",
    "It's important to keep visualizing your results. Plot the `X_1` data again, but this time color the points according to the scores calculated in A.2.3. Add a colorbar to show the numerical score corresponding to a color. A nice colormap to use is the `hot` colormap included in matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627012eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ba2ae",
   "metadata": {},
   "source": [
    "### Exercise A.2.5\n",
    "How do you think the algorithm performs? Do you observe many false positives/negatives? Explain why you think the algorithm performs well/badly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d268fdd6",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03e742",
   "metadata": {},
   "source": [
    "### Exercise A.2.6\n",
    "In the lecture `LocalOutlierFactor` should have been covered. Either do exercise B.1, or briefly describe how this method would perform here. Is one better than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aacf88",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c845140",
   "metadata": {},
   "source": [
    "## A.3 Evaluation and Active Learning\n",
    "So far, we've only done manual visual inspection of the output of our anomaly detection algorithms. In many practical use-cases, this is the only option, as we have no real labels available to us. We apply unsupervised algorithms, inspect the results, and improve our algorithm based on our findings, possibly labeling some of the (previously unlabeled) data. \n",
    "In toy examples, such as this practical, we sometmes do have labels however, and we can use these to gain some insights into how well our algorithms perform, as well as use them to replicate real-world data science practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45a30e",
   "metadata": {},
   "source": [
    "### Exercise A.3.1 - Obtain expert data\n",
    "Now we're going to take a look at how we can tune hyperparameters when we only have access to a small number of labeled samples. Normally labeling is expensive, so we'll only relabel the 10% most anomalous data after an initial unsupervised anomaly detection run. Because we only get a small set of labels after the initial run, the first pass is extremely important!\n",
    "\n",
    "1. Load new data, specifically, `data/pen-local`, and we're going to train a LOF models.\n",
    "2. Train a `LocalOutlierFactor` model with `k=3` and use it to calculate LOF scores. \n",
    "3. Save the scores resulting from this model to a variable called `y_penlocal_scores_3`. \n",
    "\n",
    "Although the setting is chosen arbitrarily, we'll treat this `k=3` as an \"expert first guess\" for hyperparameter settings!\n",
    "\n",
    "**Helpful hints**: \n",
    "- extract the labels into `y_penlocal`, they are stored under key `y` (the data itself is again under `X`)\n",
    "- Read the documentation of sklearn.neighbors `LocalOutlierFactor`\n",
    "    - For the scores, look at the negative_outlier_factor_ attribute!\n",
    "- Anomaly scores in Scikit-learn are different from scores of other, supervised, methods. In order to input them into the average precision or ROC/AUC functions, you'll need to make sure you **change the sign** so that the most anomalous samples have the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fd100",
   "metadata": {},
   "source": [
    "### Intermezzo\n",
    "Normally we would use the calculated scores in order to decide what data to present to an export, to get some labels. \n",
    "\n",
    "We of course have all the labels, stored under `y_penlocal`, but we can simulate what it would be like if we did not. Let us simulate that we do not have labels, but we want to have them! \n",
    "\n",
    "Scenario:\n",
    "- We can ask our amazing expert `E` to label a dataset\n",
    "- However, $E$ does not have unlimited time! They can only label ~600 documents. A little more is okay, but anything more than 700 is too much! This means we can only pass ~10% of our data to this expert.\n",
    "- This is of course a scenario, so in fact, `E(to_label)` just returns the labels of `to_label` from \n",
    "`y_penlocal` :p\n",
    "\n",
    "So... Now we need to be smart about which data we want to label. Maybe, a good choice for `to_label` would be to pass the top 10% of data, when judged on anomaly scores.\n",
    "\n",
    "\n",
    "\n",
    "**Helpful hint**\n",
    "- To help you, we supply you with a function `select_top_10_percent` which selects the top 10% according to the scores. \n",
    "- if you get the error that `y_penlocal` is undefined, read A.3.1 carefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee8d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def select_top_10_percent(y_scores_to_sort_by, y_labels_or_scores):\n",
    "    n_10_percent = int(len(y_labels_or_scores)/10.0)\n",
    "    \n",
    "    sort_order = np.argsort(-y_scores_to_sort_by)\n",
    "\n",
    "    y_labels_or_scores_10_percent = y_labels_or_scores[sort_order[0:n_10_percent]]\n",
    "    \n",
    "    return np.squeeze(y_labels_or_scores_10_percent)\n",
    "\n",
    "y_penlocal_3_top_10_percent = select_top_10_percent(y_penlocal_scores_3, y_penlocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e42e2",
   "metadata": {},
   "source": [
    "### Exercise A.3.2 - Hyperparameter tuning\n",
    "Now, we're going to use the small portion of labels we have to tune the hyperparameter $k$.\n",
    "\n",
    "Now do the following:\n",
    "- Make a new LOF model for $k \\in \\{1, 2, 3, \\ldots, 20\\}$ and fit it on all of `X_penlocal`.\n",
    "- For each value of $k$, calculate the ROC/AUC value using only the top 10% labels.\n",
    "    - In order to do this, you'll have to use the `select_top_10_percent` function again on the scores you calculated with the LOF model!\n",
    "- For each value of $k$, also calculate the ROC/AUC value with **all the labels**. (We'll use this to see how well optimization on a subset works)\n",
    "- Plot the ROC/AUC values belonging to the two different sets of labels as a function of $k$ and make a legend.\n",
    "\n",
    "\n",
    "**hint**: You can use the `roc_auc_score` function from Scikit-learn in order to calculate the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2fd91",
   "metadata": {},
   "source": [
    "# B: Additional Exercises\n",
    "Below you will find more exercises for week 12 of the course Data Mining and Machine Learning. You do not have to hand these in, but they are here to provide extra insight into the material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71fcd0b",
   "metadata": {},
   "source": [
    "## B.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a661c25",
   "metadata": {},
   "source": [
    "Repeat questions A.1.3 up to and including A.1.5, but use the `LocalOutlierFactor` method from scikit-learn instead of `IsolationForest`. \n",
    "To summarize:\n",
    "1. Calculate `y_1_scores` using `LocalOutlierFactor`. (no need to set `random_state`) **Important:** in sklearn, LOF has no `score_samples` function comparable to IsolationForest. Use the `negative_outlier_factor_` attribute after fitting on `X_1` instead.\n",
    "2. Plot the scores and make a color bar\n",
    "3. Reflect on how the algorithm performs: \"How do you think the algorithm performs? Do you observe many false positives/negatives? Explain why you think the algorithm performs well/badly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57adf4d",
   "metadata": {},
   "source": [
    "YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc490e0",
   "metadata": {},
   "source": [
    "## B.2\n",
    "\n",
    "### Theory exercise B.2.1\n",
    "Look at the Intermezzo for A.3\n",
    "- Why don't we just take the first 10% or a random sample of the data?\n",
    "- What would happen if we only get 1 class, in the extreme case?\n",
    "- How likely is something an anomalaly on this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517f1e0",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce8008",
   "metadata": {},
   "source": [
    "### Theory exercise B.2.2\n",
    "Using your final implementation for A.3.2. answer the following questions, using code to supplement them where needed:\n",
    "1. What is the optimal value of $k$ when you optimize on the top 10% labels?\n",
    "2. What is the optimal value of $k$ when you optimize on all data?\n",
    "3. The optimal values of $k$ are different, can you explain why? \n",
    "4. When you compare the two score graphs, why do you find the optimal values at different places? \n",
    "5. Can you explain why the value of $k$ when optimized on the top 10% labels is so close to the initial guess of $k=3$? Do you think the initial guess was good?\n",
    "6. How close does the model optimized on the top 10% get to the best model when using all the labels? Explain in terms of ROC/AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49582ccf",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44387d",
   "metadata": {},
   "source": [
    "# Handing in\n",
    "Hand in the filled-in notebook:\n",
    "- Make sure your names and student-numbers are mentioned at the appropriate location at the top of this file.\n",
    "- Make sure you wrote a serious attempt for all exercises in part A \n",
    "- Make sure the notebook is named groupnumber_assignment12.ipynb\n",
    "\n",
    "When you are confident all of this is correctly done, one of you can hand in the exercises in Brightspace!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
